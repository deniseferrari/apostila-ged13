<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 10 Estimação Pontual | GED-13: Probabilidade e Estatística</title>
  <meta name="description" content="Apostila do curso de GED-13: Probabilidade e Estatística." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 10 Estimação Pontual | GED-13: Probabilidade e Estatística" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apostila do curso de GED-13: Probabilidade e Estatística." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 10 Estimação Pontual | GED-13: Probabilidade e Estatística" />
  
  <meta name="twitter:description" content="Apostila do curso de GED-13: Probabilidade e Estatística." />
  

<meta name="author" content="Prof. Denise Beatriz Ferrari" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distribuições-amostrais.html"/>
<link rel="next" href="intervalos-de-confiança.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Probabilidade e Estatística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Objetivos do Curso</a></li>
<li class="chapter" data-level="1" data-path="introdução.html"><a href="introdução.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="introdução.html"><a href="introdução.html#estatística-e-o-raciocínio-científico"><i class="fa fa-check"></i><b>1.1</b> Estatística e o Raciocínio Científico</a></li>
<li class="chapter" data-level="1.2" data-path="introdução.html"><a href="introdução.html#o-que-é-estatística"><i class="fa fa-check"></i><b>1.2</b> O que é Estatística?</a></li>
<li class="chapter" data-level="1.3" data-path="introdução.html"><a href="introdução.html#o-papel-da-probabilidade-em-estatística"><i class="fa fa-check"></i><b>1.3</b> O Papel da Probabilidade em Estatística</a></li>
<li class="chapter" data-level="1.4" data-path="introdução.html"><a href="introdução.html#elementos-fundamentais-em-estatística"><i class="fa fa-check"></i><b>1.4</b> Elementos Fundamentais em Estatística</a><ul>
<li class="chapter" data-level="" data-path="introdução.html"><a href="introdução.html#população-e-amostra"><i class="fa fa-check"></i>População e Amostra</a></li>
<li class="chapter" data-level="" data-path="introdução.html"><a href="introdução.html#variáveis"><i class="fa fa-check"></i>Variáveis</a></li>
<li class="chapter" data-level="" data-path="introdução.html"><a href="introdução.html#dados-e-fontes-de-dados"><i class="fa fa-check"></i>Dados e Fontes de Dados</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introdução.html"><a href="introdução.html#tipos-de-problemas"><i class="fa fa-check"></i><b>1.5</b> Tipos de Problemas</a></li>
<li class="chapter" data-level="1.6" data-path="introdução.html"><a href="introdução.html#o-processo-de-análise-de-dados"><i class="fa fa-check"></i><b>1.6</b> O Processo de Análise de Dados</a></li>
<li class="chapter" data-level="1.7" data-path="introdução.html"><a href="introdução.html#métodos-para-exploração-resumo-e-descrição-de-dados"><i class="fa fa-check"></i><b>1.7</b> Métodos para Exploração, Resumo e Descrição de Dados</a><ul>
<li class="chapter" data-level="" data-path="introdução.html"><a href="introdução.html#análise-exploratória-de-dados-exploratory-data-analysis-eda"><i class="fa fa-check"></i>Análise Exploratória de Dados (“Exploratory Data Analysis”, EDA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html"><i class="fa fa-check"></i><b>2</b> Introdução à Teoria de Probabilidades</a><ul>
<li class="chapter" data-level="2.1" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#breve-histórico"><i class="fa fa-check"></i><b>2.1</b> Breve Histórico</a><ul>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#chance-e-incerteza"><i class="fa fa-check"></i>Chance e Incerteza</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#jogos-de-azar"><i class="fa fa-check"></i>Jogos de Azar</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#origem-da-teoria-matemática-de-probabilidades"><i class="fa fa-check"></i>Origem da Teoria Matemática de Probabilidades</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#formalização-matemática"><i class="fa fa-check"></i>Formalização Matemática</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#definições-iniciais"><i class="fa fa-check"></i><b>2.2</b> Definições Iniciais</a><ul>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#experimento-aleatório"><i class="fa fa-check"></i>Experimento Aleatório</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#espaço-amostral-e-evento"><i class="fa fa-check"></i>Espaço Amostral e Evento</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#lei-de-probabilidade"><i class="fa fa-check"></i>Lei de Probabilidade</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#interpretações-de-probabilidade"><i class="fa fa-check"></i><b>2.3</b> Interpretações de Probabilidade</a><ul>
<li><a href="introdução-à-teoria-de-probabilidades.html#interpretação-clássica-a-priori-laplace-1812">Interpretação Clássica (<em>a priori</em>): Laplace, 1812</a></li>
<li><a href="introdução-à-teoria-de-probabilidades.html#interpretação-empírica-ou-de-frequência-relativa-a-posteriori-richard-v.-mises-1919">Interpretação Empírica ou de Frequência Relativa (<em>a posteriori</em>): Richard V. Mises, 1919</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#interpretação-subjetiva"><i class="fa fa-check"></i>Interpretação Subjetiva</a></li>
<li class="chapter" data-level="" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#resumo"><i class="fa fa-check"></i>Resumo</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#definição-axiomática"><i class="fa fa-check"></i><b>2.4</b> Definição Axiomática</a></li>
<li class="chapter" data-level="2.5" data-path="introdução-à-teoria-de-probabilidades.html"><a href="introdução-à-teoria-de-probabilidades.html#revisitando-o-paradoxo-de-de-méré-o-problema-dos-dados"><i class="fa fa-check"></i><b>2.5</b> Revisitando o Paradoxo de De Méré: O Problema dos Dados</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html"><i class="fa fa-check"></i><b>3</b> Probabilidade Condicional e Independência</a><ul>
<li class="chapter" data-level="3.1" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#probabilidade-condicional"><i class="fa fa-check"></i><b>3.1</b> Probabilidade Condicional</a><ul>
<li class="chapter" data-level="" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#propriedades"><i class="fa fa-check"></i>Propriedades</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#independência-de-eventos"><i class="fa fa-check"></i><b>3.2</b> Independência de Eventos</a><ul>
<li class="chapter" data-level="" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#propriedades-1"><i class="fa fa-check"></i>Propriedades</a></li>
<li class="chapter" data-level="" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#independência-condicional"><i class="fa fa-check"></i>Independência Condicional</a></li>
<li class="chapter" data-level="" data-path="probabilidade-condicional-e-independência.html"><a href="probabilidade-condicional-e-independência.html#eventos-independentes-x-eventos-mutuamente-exclusivos"><i class="fa fa-check"></i>Eventos Independentes x Eventos Mutuamente Exclusivos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="teoremas-fundamentais-da-probabilidade.html"><a href="teoremas-fundamentais-da-probabilidade.html"><i class="fa fa-check"></i><b>4</b> Teoremas Fundamentais da Probabilidade</a><ul>
<li class="chapter" data-level="4.1" data-path="teoremas-fundamentais-da-probabilidade.html"><a href="teoremas-fundamentais-da-probabilidade.html#teorema-da-probabilidade-total-dividir-para-conquistar"><i class="fa fa-check"></i><b>4.1</b> Teorema da Probabilidade Total: …dividir para conquistar!</a></li>
<li class="chapter" data-level="4.2" data-path="teoremas-fundamentais-da-probabilidade.html"><a href="teoremas-fundamentais-da-probabilidade.html#teorema-de-bayes-aprendendo-pela-experiência"><i class="fa fa-check"></i><b>4.2</b> Teorema de Bayes: …aprendendo pela experiência</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html"><i class="fa fa-check"></i><b>5</b> Variáveis Aleatórias e Distribuições</a><ul>
<li class="chapter" data-level="5.1" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#variáveis-aleatórias"><i class="fa fa-check"></i><b>5.1</b> Variáveis Aleatórias</a><ul>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#definição-caso-unidimensional"><i class="fa fa-check"></i>Definição (caso unidimensional)</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#tipos-de-variáveis-aleatórias"><i class="fa fa-check"></i>Tipos de Variáveis Aleatórias</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#distribuições-de-probabilidade"><i class="fa fa-check"></i><b>5.2</b> Distribuições de Probabilidade</a><ul>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#função-distribuição-de-probabilidade-fdp-caso-discreto"><i class="fa fa-check"></i>Função Distribuição de Probabilidade (fdp): caso discreto</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#função-distribuição-de-probabilidade-fdp-caso-contínuo"><i class="fa fa-check"></i>Função Distribuição de Probabilidade (fdp): caso contínuo</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#função-distribuição-acumulada-fda"><i class="fa fa-check"></i>Função Distribuição Acumulada (FDA)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#valor-esperado-e-variância"><i class="fa fa-check"></i><b>5.3</b> Valor Esperado e Variância</a><ul>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#valor-esperado"><i class="fa fa-check"></i>Valor Esperado</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#o-problema-dos-pontos-e-a-aposta-de-pascal"><i class="fa fa-check"></i>O Problema dos Pontos e a Aposta de Pascal</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#variância"><i class="fa fa-check"></i>Variância</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#desvio-padrão"><i class="fa fa-check"></i>Desvio-padrão</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#momentos"><i class="fa fa-check"></i><b>5.4</b> Momentos</a><ul>
<li><a href="variáveis-aleatórias-e-distribuições.html#assimetria-skewness-e-excesso-kurtosis">Assimetria (<em>skewness</em>) e Excesso (<em>kurtosis</em>)</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#desigualdades-de-markov-e-chebyshev"><i class="fa fa-check"></i><b>5.5</b> Desigualdades de Markov e Chebyshev</a><ul>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#desigualdade-de-markov"><i class="fa fa-check"></i>Desigualdade de Markov</a></li>
<li class="chapter" data-level="" data-path="variáveis-aleatórias-e-distribuições.html"><a href="variáveis-aleatórias-e-distribuições.html#desigualdade-de-chebyshev"><i class="fa fa-check"></i>Desigualdade de Chebyshev</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><i class="fa fa-check"></i><b>6</b> Modelos Probabilísticos: Distribuições Associadas a Processos de Bernoulli</a><ul>
<li class="chapter" data-level="6.1" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#o-experimento-de-bernoulli"><i class="fa fa-check"></i><b>6.1</b> O Experimento de Bernoulli</a></li>
<li class="chapter" data-level="6.2" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-de-bernoulli"><i class="fa fa-check"></i><b>6.2</b> Distribuição de Bernoulli</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#resumo-2"><i class="fa fa-check"></i>Resumo</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-binomial"><i class="fa fa-check"></i><b>6.3</b> Distribuição Binomial</a></li>
<li class="chapter" data-level="6.4" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#um-problema-de-tomada-de-decisão"><i class="fa fa-check"></i><b>6.4</b> Um Problema de Tomada de Decisão</a></li>
<li class="chapter" data-level="6.5" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-geométrica"><i class="fa fa-check"></i><b>6.5</b> Distribuição Geométrica</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#propriedade-de-ausência-de-memória"><i class="fa fa-check"></i>Propriedade de Ausência de Memória</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#outras-distribuições"><i class="fa fa-check"></i><b>6.6</b> Outras Distribuições</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-binomial-negativa-ou-distribuição-de-pascal"><i class="fa fa-check"></i>Distribuição Binomial Negativa (ou Distribuição de Pascal)</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-multinomial"><i class="fa fa-check"></i>Distribuição Multinomial</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-multinomial-negativa"><i class="fa fa-check"></i>Distribuição Multinomial Negativa</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-hipergeométrica"><i class="fa fa-check"></i>Distribuição Hipergeométrica</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#distribuição-hipergeométrica-negativa"><i class="fa fa-check"></i>Distribuição Hipergeométrica Negativa</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-bernoulli.html#resumo-4"><i class="fa fa-check"></i>Resumo</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><i class="fa fa-check"></i><b>7</b> Modelos Probabilísticos: Distribuições Associadas a Processos de Poisson</a><ul>
<li class="chapter" data-level="7.1" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#uma-aproximação-para-a-distribuição-binomial"><i class="fa fa-check"></i><b>7.1</b> Uma aproximação para a Distribuição Binomial</a></li>
<li class="chapter" data-level="7.2" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#distribuição-de-poisson"><i class="fa fa-check"></i><b>7.2</b> Distribuição de Poisson</a></li>
<li class="chapter" data-level="7.3" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#o-processo-de-poisson"><i class="fa fa-check"></i><b>7.3</b> O Processo de Poisson</a></li>
<li class="chapter" data-level="7.4" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#distribuição-exponencial"><i class="fa fa-check"></i><b>7.4</b> Distribuição Exponencial</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#propriedade-de-ausência-de-memória-1"><i class="fa fa-check"></i>Propriedade de Ausência de Memória</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#distribuição-de-weibull"><i class="fa fa-check"></i>Distribuição de Weibull:</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html"><a href="modelos-probabilísticos-distribuições-associadas-a-processos-de-poisson.html#distribuição-gama"><i class="fa fa-check"></i><b>7.5</b> Distribuição Gama</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html"><i class="fa fa-check"></i><b>8</b> Modelos Probabilísticos: Distribuição Normal</a><ul>
<li class="chapter" data-level="8.1" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#distribuição-normal"><i class="fa fa-check"></i><b>8.1</b> Distribuição Normal</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#mais-uma-aproximação-para-a-distribuição-binomial"><i class="fa fa-check"></i>…(Mais) Uma Aproximação para a Distribuição Binomial</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#cálculo-de-probabilidades"><i class="fa fa-check"></i>Cálculo de Probabilidades</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#padronização-1"><i class="fa fa-check"></i>Padronização</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#regra-empírica"><i class="fa fa-check"></i>Regra Empírica</a></li>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#coeficiente-de-variação"><i class="fa fa-check"></i>Coeficiente de Variação</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#aproximação-para-distribuições-discretas"><i class="fa fa-check"></i><b>8.2</b> Aproximação para Distribuições Discretas</a><ul>
<li class="chapter" data-level="" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#aproximação-para-a-distribuição-binomial"><i class="fa fa-check"></i>Aproximação para a Distribuição Binomial</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="modelos-probabilísticos-distribuição-normal.html"><a href="modelos-probabilísticos-distribuição-normal.html#métodos-descritivos-para-avaliar-normalidade"><i class="fa fa-check"></i><b>8.3</b> Métodos Descritivos para Avaliar Normalidade</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html"><i class="fa fa-check"></i><b>9</b> Distribuições Amostrais</a><ul>
<li class="chapter" data-level="9.1" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#introdução-à-inferência-estatística"><i class="fa fa-check"></i><b>9.1</b> Introdução à Inferência Estatística</a><ul>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#inferência-estatística"><i class="fa fa-check"></i>Inferência Estatística</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#amostras-e-distribuições-amostrais"><i class="fa fa-check"></i><b>9.2</b> Amostras e Distribuições Amostrais</a><ul>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#amostra-aleatória"><i class="fa fa-check"></i>Amostra Aleatória</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#parâmetos-vs.-estatísticas"><i class="fa fa-check"></i>Parâmetos vs. Estatísticas</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-amostral"><i class="fa fa-check"></i>Distribuição Amostral</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-da-média-amostral"><i class="fa fa-check"></i><b>9.3</b> Distribuição da Média Amostral</a><ul>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#propriedades-da-média-amostral"><i class="fa fa-check"></i>Propriedades da Média Amostral</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#lei-dos-grandes-números"><i class="fa fa-check"></i>Lei dos Grandes Números</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#teorema-do-limite-central"><i class="fa fa-check"></i><b>9.4</b> Teorema do Limite Central</a></li>
<li class="chapter" data-level="9.5" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuições-amostrais-associadas-a-populações-normais"><i class="fa fa-check"></i><b>9.5</b> Distribuições Amostrais Associadas a Populações Normais</a><ul>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-qui-quadrado"><i class="fa fa-check"></i>Distribuição Qui-Quadrado</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-t-student"><i class="fa fa-check"></i>Distribuição t-Student</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#aproximando-distribuições-amostrais-via-simulação-de-monte-carlo"><i class="fa fa-check"></i><b>9.6</b> Aproximando Distribuições Amostrais via Simulação de Monte Carlo</a><ul>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-aproximada-da-mediana-amostral"><i class="fa fa-check"></i>Distribuição Aproximada da Mediana Amostral</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-aproximada-do-desvio-padrão-amostral"><i class="fa fa-check"></i>Distribuição Aproximada do Desvio-Padrão Amostral</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-aproximada-da-variância-amostral"><i class="fa fa-check"></i>Distribuição Aproximada da Variância Amostral</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-aproximada-do-mad-desvio-mediano-absoluto"><i class="fa fa-check"></i>Distribuição Aproximada do MAD (Desvio Mediano Absoluto)</a></li>
<li class="chapter" data-level="" data-path="distribuições-amostrais.html"><a href="distribuições-amostrais.html#distribuição-aproximada-da-amplitude-inter-quartis-iqr"><i class="fa fa-check"></i>Distribuição Aproximada da Amplitude Inter-Quartis (IQR)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimação-pontual.html"><a href="estimação-pontual.html"><i class="fa fa-check"></i><b>10</b> Estimação Pontual</a><ul>
<li class="chapter" data-level="10.1" data-path="estimação-pontual.html"><a href="estimação-pontual.html#estimador-e-estimativa"><i class="fa fa-check"></i><b>10.1</b> Estimador e Estimativa</a></li>
<li class="chapter" data-level="10.2" data-path="estimação-pontual.html"><a href="estimação-pontual.html#propriedades-de-estimadores"><i class="fa fa-check"></i><b>10.2</b> Propriedades de Estimadores</a><ul>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#não-tendeciosidade-exatidão"><i class="fa fa-check"></i>Não-Tendeciosidade (exatidão)</a></li>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#eficiência-precisão"><i class="fa fa-check"></i>Eficiência (precisão)</a></li>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#consistência"><i class="fa fa-check"></i>Consistência</a></li>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#erro-médio-quadrático"><i class="fa fa-check"></i>Erro Médio Quadrático</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="estimação-pontual.html"><a href="estimação-pontual.html#métodos-clássicos-de-estimação-de-parâmetros"><i class="fa fa-check"></i><b>10.3</b> Métodos Clássicos de Estimação de Parâmetros</a><ul>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#método-dos-momentos"><i class="fa fa-check"></i>Método dos Momentos</a></li>
<li class="chapter" data-level="" data-path="estimação-pontual.html"><a href="estimação-pontual.html#método-da-máxima-verossimilhança"><i class="fa fa-check"></i>Método da Máxima Verossimilhança</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="intervalos-de-confiança.html"><a href="intervalos-de-confiança.html"><i class="fa fa-check"></i><b>11</b> Intervalos de Confiança</a><ul>
<li class="chapter" data-level="11.1" data-path="intervalos-de-confiança.html"><a href="intervalos-de-confiança.html#estimação-por-intervalos"><i class="fa fa-check"></i><b>11.1</b> Estimação por Intervalos</a></li>
<li class="chapter" data-level="11.2" data-path="intervalos-de-confiança.html"><a href="intervalos-de-confiança.html#procedimento-para-construção-de-ics"><i class="fa fa-check"></i><b>11.2</b> Procedimento para Construção de IC’s</a><ul>
<li><a href="intervalos-de-confiança.html#caso-1-ic-para-mu-com-sigma2-conhecida">CASO 1: IC para <span class="math inline">\(\mu\)</span> com <span class="math inline">\(\sigma^2\)</span> conhecida</a></li>
<li><a href="intervalos-de-confiança.html#caso-2.1-ic-para-mu-com-sigma2-desconhecida">CASO 2.1: IC para <span class="math inline">\(\mu\)</span> com <span class="math inline">\(\sigma^2\)</span> desconhecida</a></li>
<li><a href="intervalos-de-confiança.html#caso-2.2-ic-para-mu-com-sigma2-desconhecida-amostras-grandes">CASO 2.2: IC para <span class="math inline">\(\mu\)</span> com <span class="math inline">\(\sigma^2\)</span> desconhecida (amostras grandes)</a></li>
<li><a href="intervalos-de-confiança.html#caso-3-ic-para-p-proporção-populacional">CASO 3: IC para <span class="math inline">\(p\)</span> (proporção populacional)</a></li>
<li><a href="intervalos-de-confiança.html#caso-3-ic-para-sigma2">CASO 3: IC para <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html"><i class="fa fa-check"></i><b>12</b> Testes de Hipóteses</a><ul>
<li class="chapter" data-level="12.1" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#formulação-de-hipóteses-estatísticas"><i class="fa fa-check"></i><b>12.1</b> Formulação de Hipóteses Estatísticas</a></li>
<li class="chapter" data-level="12.2" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#estatística-do-teste"><i class="fa fa-check"></i><b>12.2</b> Estatística do Teste</a></li>
<li class="chapter" data-level="12.3" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#erros-de-decisão"><i class="fa fa-check"></i><b>12.3</b> Erros de Decisão</a></li>
<li class="chapter" data-level="12.4" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#região-crítica"><i class="fa fa-check"></i><b>12.4</b> Região Crítica</a></li>
<li class="chapter" data-level="12.5" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#valor-p"><i class="fa fa-check"></i><b>12.5</b> Valor-p</a></li>
<li class="chapter" data-level="" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#qual-a-probabilidade-de-cometer-erro-do-tipo-ii"><i class="fa fa-check"></i>Qual a probabilidade de cometer erro do tipo II?</a></li>
<li class="chapter" data-level="12.6" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#poder-do-teste"><i class="fa fa-check"></i><b>12.6</b> Poder do Teste</a></li>
<li class="chapter" data-level="12.7" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#resumo-gráfico"><i class="fa fa-check"></i><b>12.7</b> Resumo Gráfico</a></li>
<li class="chapter" data-level="12.8" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#testes-mono--e-bi-caudais"><i class="fa fa-check"></i><b>12.8</b> Testes Mono- e Bi-Caudais</a><ul>
<li class="chapter" data-level="" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#região-de-rejeição-para-um-teste-bi-caudal"><i class="fa fa-check"></i>Região de Rejeição para um Teste Bi-caudal</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#procedimento-para-testes-de-hipóteses-utilizando-o-nível-de-significância"><i class="fa fa-check"></i><b>12.9</b> Procedimento para Testes de Hipóteses (utilizando o nível de significância)</a></li>
<li class="chapter" data-level="12.10" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#procedimento-para-testes-de-hipóteses-utilizando-valor-p"><i class="fa fa-check"></i><b>12.10</b> Procedimento para Testes de Hipóteses (utilizando valor-p)</a></li>
<li class="chapter" data-level="12.11" data-path="testes-de-hipóteses.html"><a href="testes-de-hipóteses.html#ic-vs-th"><i class="fa fa-check"></i><b>12.11</b> IC vs TH</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">GED-13: Probabilidade e Estatística</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimação-pontual" class="section level1">
<h1><span class="header-section-number">Capítulo 10</span> Estimação Pontual</h1>
<p>No capítulo anterior, vimos que um conjunto de observações pode ser entendido como uma realização de uma amostra aleatória coletada a partir de uma população com certa distribuição de probabilidade. E, considerando que esse conjunto de dados constitui uma amostra representativa da população em questão, podemos utilizá-la para fazer inferências.</p>
<p>Neste sentido, realizamos inferência para transformar dados em conhecimentos, onde o conhecimento é comumente representado em termos de entidades que não estão presentes nos dados, mas estão presentes nos modelos que utilizamos para interpretar os dados.</p>
<p>Em estatística clássica, assumimos que o estado da natureza (representado através de parâmetros de um modelo) é fixo, embora desconhecido. Portanto, na abordagem clássica, podemos realizar inferência de duas maneiras:
(i) estimando o valor desconhecido de uma quantidade populacional, ou<br />
(ii) tomando uma decisão com relação a uma conjectura a respeito de uma quantidade populacional.</p>
<p>Neste capítulo, trataremos do problema de estimação que, na estatística clássica, se baseia em duas perguntas fundamentais:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Como determinamos estimadores?<br />
</li>
<li>Como avaliamos estimadores?</li>
</ol>
</blockquote>
<p>Veremos os conceitos de estimador e estimativa, bem como discutiremos algumas propriedades desejáveis dos estimadores para, finalmente, aprendermos como construir estimadores para certas características populacionais.</p>
<p>Para isso, consideremos o seguinte exemplo:</p>
<p>Suponha que observamos o processo de chegada de clientes a um café. E, que mais especificamente, estamos interessados em determinar: (i) taxa de chegada dos clientes; e (ii) o percentual de tempo em que nenhum novo cliente chega ao estabelecimento ( <span class="math inline">\(p_o\)</span> ).</p>
<p>Vimos anteriormente que, se as chegadas ocorrem completamente ao acaso no tempo e,
guardadas as hipóteses de homogeneidade e independência, o processo de chegada pode ser modelado como um processo de Poisson.</p>
<p>Portanto, definindo a va.<br />
<span class="math inline">\(X\)</span> = número de chegadas ao café (durante um certo intervalo de tempo),</p>
<p>então</p>
<p><span class="math inline">\(X \sim Pois( \lambda )\)</span>, com <span class="math inline">\(\lambda\)</span> desconhecido.</p>
<p>Utilizando o modelo de Poisson:</p>
<ul>
<li>a taxa de chegada é dada pelo parâmetro <span class="math inline">\(\lambda\)</span>, que corresponde ao valor esperado da distribuição;<br />
</li>
<li><span class="math inline">\(p_o\)</span> corresponde à probabilidade P[X = 0] = <span class="math inline">\(e^{-\lambda}\)</span>.</li>
</ul>
<p>Inicia-se, então, a contagem do tempo e observa-se o processo de chegada por um certo período, registrando-se os valores numéricos</p>
<p><span class="math display">\[x_1, x_2, \ldots, x_n,\]</span></p>
<p>onde:</p>
<p><span class="math inline">\(x_i=\)</span> número <strong>observado</strong> de chegadas no <span class="math inline">\(i\)</span>-ésimo instante, a partir do início da contagem do tempo.</p>
<p>Este exemplo ilustra a situação geral em que o conjunto de dados obtido</p>
<p><span class="math display">\[x_1, x_2, \ldots, x_n,\]</span></p>
<p>é modelado como a <strong>realização de uma amostra aleatória</strong></p>
<p><span class="math display">\[X_1, X_2, \ldots, X_n,\]</span></p>
<p>retirada de uma população completamente determinada por um conjunto de parâmetros.</p>
<p>Note que antes de os valores serem efetivamente observados, as observações que compõe uma a.a. genérica de tamanho <span class="math inline">\(n\)</span> são representadas por letras maiúsculas.</p>
<p>Precisamos, então, <strong>estimar</strong>, com base nas quantidades observadas:</p>
<ul>
<li>o parâmetro <span class="math inline">\(\lambda\)</span>, e<br />
</li>
<li>uma função do parâmetro, <span class="math inline">\(p_o = e^{-\lambda}\)</span></li>
</ul>
<div id="estimador-e-estimativa" class="section level2">
<h2><span class="header-section-number">10.1</span> Estimador e Estimativa</h2>
<p>De maneira geral, suponha que desejamos estimar uma quantidade ou parâmetro populacional representado por <span class="math inline">\(\theta\)</span>. Por exemplo, <span class="math inline">\(\theta\)</span> poderia ser a média populacional <span class="math inline">\(\mu\)</span>, ou a variância populacional <span class="math inline">\(\sigma^2\)</span>, ou ainda uma outra quantidade associada a essa população, como a probabilidade <span class="math inline">\(p_o\)</span> no exemplo anterior. Sendo assim, se <span class="math inline">\(\theta\)</span> for o parâmetro populacional de interesse, então:</p>
<p>Um <strong>ESTIMADOR</strong> de <span class="math inline">\(\theta\)</span>, representado por <span class="math inline">\(\hat\Theta\)</span>, é uma função dos valores conceituais, não-observados, de uma a.a.. O estimador é uma “regra” que nos ensina como utilizar as observações da a.a. para calcular um valor numérico que estima o parâmetro populacional em questão.</p>
<p>Já uma <strong>ESTIMATIVA</strong>, representada por <span class="math inline">\(\hat\theta\)</span>, corresponde a um valor numérico calculado a partir dos valores efetivamente observados de uma a.a. em particular, utilizando a regra prescrita pelo estimador. Diferentes amostras aleatórias resultarão em diferentes estimativas para um mesmo parâmetro populacional, quando se utiliza o mesmo estimador. Qualquer que seja a regra ou o estimador utilizado, o resultado da estimativa depende apenas dos valores observados na amostra.</p>
<p>Imagine, por exemplo, que um alienígena deseja estimar a altura média dos seres humanos. Neste caso, o parâmetro que se deseja estimar é a média populacional <span class="math inline">\(\mu\)</span>. Podemos eleger como estimador a média amostral <span class="math inline">\(\bar{X}_n\)</span>. Isto significa que estimaremos a altura média populacional calculando a média aritmética dos valores observados na amostra; esta é a regra prescrita pelo estimador “média amostral”. Então, coletamos uma amostra aleatória de, digamos, 20 indivíduos dessa população, calculamos a média aritmética dessas 20 observações, obtendo uma estimativa de 1,72m para a altura média populacional; uma outra amostra aleatória distinta, também contendo 20 observações, poderia nos ter dado uma estimativa de, por exemplo, 1,68m para a altura média populacional.</p>
<p>Cada estimativa corresponde a um valor numérico; portanto, a estimativa <span class="math inline">\(\hat\theta\)</span> representa uma realização do estimador <span class="math inline">\(\hat\Theta\)</span>. Já um estimador é uma estatística, isto é, uma função dos valores não observados de uma a.a.. Antes de os dados serem coletados, os valores da amostra aleatória são considerados v.a.’s retirados de maneira independente de uma distribuição comum. E, como qualquer função de v.a.’s é uma v.a., um estimador é também uma v.a. Portanto, um estimador possui uma distribuição amostral. Assim, a distribuição amostral do estimador descreve de maneira completa seu comportamento aleatório. Sabemos, pelo TLC, por exemplo, que o estimador média amostral tem distribuição aproximadamente normal para amostras de tamanho elevado. A ideia é, então, escolher um estimador <span class="math inline">\(\hat\Theta\)</span> capaz de nos dar boas indicações (ou estimativas) a respeito do valor “real” do parâmetro populacional <span class="math inline">\(\theta\)</span>, com base nas informações disponíveis em uma amostra.</p>
<div id="estimando-lambda" class="section level4 unnumbered">
<h4>Estimando <span class="math inline">\(\lambda\)</span></h4>
<p>Retomando o exemplo do café, como podemos proceder para estimar <span class="math inline">\(\lambda\)</span>?</p>
<p>Bem, sabemos que a distribuição de Poisson tem média e variância iguais ao parâmetro populacional que a define, <span class="math inline">\(\lambda\)</span>. Já que a média populacional vale <span class="math inline">\(\lambda\)</span>, a Lei dos Grandes Números sugere que a média amostral seja um estimador natural para a média populacional. Por outro lado, o parâmetro <span class="math inline">\(\lambda\)</span> também representa variância populacional no modelo de Poisson. Portanto, as características da distribuição de Poisson sugerem que temos dois estimadores naturais para <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li><p>média amostral:
<span class="math display">\[\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\]</span></p></li>
<li><p>variância amostral:
<span class="math display">\[S_n^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X}_n)^2\]</span></p></li>
</ul>
<p>E as opções não param por aí… Outros estimadores também seriam possíveis! De toda forma, consideraremos apenas esses dois candidatos.</p>
</div>
<div id="estimando-p_o" class="section level4 unnumbered">
<h4>Estimando <span class="math inline">\(p_o\)</span></h4>
<p>Estamos interessados também em outra quantidade populacional, <span class="math inline">\(p_o\)</span>, que corresponde à probabilidade de nenhuma ocorrência. Segundo o modelo de Poisson, esta probabilidade é dada por uma função do parâmetro <span class="math inline">\(\lambda\)</span>, e vale <span class="math inline">\(e^{-\lambda}\)</span>.</p>
<p>Sendo assim, também temos dois estimadores naturais para <span class="math inline">\(p_o\)</span>:</p>
<ul>
<li><p>frequência relativa de zeros observados na a.a.:
<span class="math display">\[\frac{\textsf{no. de observações na a.a. em que }X_i = 0}{n}\]</span></p></li>
<li><p>uma função do estimador de <span class="math inline">\(\lambda\)</span> (já que utilizamos a média amostral <span class="math inline">\(\bar{X}_n\)</span> para estimar a média populacional <span class="math inline">\(\lambda\)</span>, podemos estimar <span class="math inline">\(p_o\)</span> substituindo o parâmetro populacional <span class="math inline">\(\lambda\)</span> na expressão de <span class="math inline">\(p_o\)</span> pelo estimador média amostral):
<span class="math display">\[e^{-\bar{X}_n}\]</span></p></li>
</ul>
<p>Aqui também seria possível escolher outros estimadores, mas nos limitaremos a esses dois.</p>
<p>Temos inúmeras opções disponíveis de estimadores para os parâmetros populacionais de interesse; podemos empregar até mesmo funções de estimadores. Algumas questões importantes são as seguintes:</p>
<ul>
<li><em>Como escolher um estimador adequado?</em><br />
</li>
<li><em>Quando um estimador é melhor do que outro?</em><br />
</li>
<li><em>Existe um estimador que seja o “melhor de todos”?</em><br />
</li>
<li><em>Podemos determinar qual das estimativas obtidas a partir de uma mesma a.a., utilizando diferentes estimadores, se aproxima mais do valor “real” do parâmetro populacional de interesse?</em></li>
</ul>
<p>No nosso exemplo, desejamos estimar o parâmetro <span class="math inline">\(\lambda\)</span> e, para isso, elegemos os estimadores média amostral e variância amostral para obter estimativas para o valor de <span class="math inline">\(\lambda\)</span> a partir dos valores observados para uma certa a.a.. Então, suponha que tenhamos registrado 30 observações e que tenhamos obtido as estimativas média amostral = 2,03 e variância amostral = 1,62 para essa amostra. Será que é possível determinar qual dessas estimativas (2,03 ou 1,62) se aproxima mais do valor real e desconhecido do parâmetro <span class="math inline">\(\lambda\)</span>?</p>
<p>A resposta a esta pergunta é NÃO! As observações que compõe uma a.a. e, consequentemente, as estimativas obtidas a partir dela, são sujeitas à aleatoriedade. Portanto, não é possível afirmar com certeza qual dos valores estimados é mais próximo do valor real de lambda!</p>
<p>É, possível, no entanto, fazer afirmações probabilísticas a respeito da distância dessas estimativas ao valor real do parâmetro de interesse. Para isso, é necessário investigar o comportamento aleatório dos estimadores, através do conhecimento de sua distribuição amostral. E assim, sermos capazes de identificar se possuem certas características consideradas desejáveis para um estimador. É isso o que veremos a seguir.</p>
</div>
</div>
<div id="propriedades-de-estimadores" class="section level2">
<h2><span class="header-section-number">10.2</span> Propriedades de Estimadores</h2>
<p>Há ocasiões em que muitos estimadores alternativos podem ser utilizados para estimar uma mesma quantidade populacional desconhecida. É necessário, portanto, estabelecer alguns critérios para comparar os estimadores e decidir se algum ou qual deles possui características desejáveis para um estimador. As propriedades de um estimador ideal que veremos a seguir foram conceitos introduzidos por sir Ronald Fisher na primeira metade do século XX.</p>
<p>A fim de discutir tais as propriedades desejadas de estimadores, vamos retomar nosso exemplo do café. Suponha que observamos as chegadas ao café que ocorreram durante 30 minutos e registramos o número de chegadas a cada minuto. Assim, obtemos um conjunto de dados, que corresponde a uma realização de uma a.a. <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, de tamanho <span class="math inline">\(n=30\)</span> de uma população com distribuição de Poisson com parâmetro ( <span class="math inline">\(\lambda\)</span> ). Com base nesta amostra, queremos estimar as quantidades populacionais de interesse: <span class="math inline">\(\lambda\)</span> e <span class="math inline">\(p_o\)</span></p>
<div id="estimando-lambda-1" class="section level4 unnumbered">
<h4>Estimando <span class="math inline">\(\lambda\)</span></h4>
<p>Temos dois estimadores candidatos para o parâmetro <span class="math inline">\(\lambda\)</span>: média amostral e variância amostral.
Qual deles devemos escolher?</p>
<p>Vamos utilizar simulação para nos ajudar a escolher os estimadores para os parâmetros desejados:</p>
<p>A título de ilustração, vamos supor que o valor real do parâmetro seja <span class="math inline">\(\lambda = \ln10\)</span>;
então, vamos fingir que não conhecemos essa informação e vamos fazer um pequeno jogo de adivinhação:</p>
<ul>
<li>Amostramos 30 observações a partir da população Poisson com parâmetro <span class="math inline">\(\lambda = \ln10\)</span>;<br />
</li>
<li>Calculamos os valores de <span class="math inline">\(\bar{X}_n\)</span> e <span class="math inline">\(S_n^2\)</span> para a amostra coletada; e<br />
</li>
<li>Repetimos os dois passos anteriores um número muito grande de vezes.</li>
</ul>
<p>O que estamos fazendo dessa maneira? Estamos obtendo um número muito grande de valores de <span class="math inline">\(\bar{X}_n\)</span> e <span class="math inline">\(S_n^2\)</span> a fim de que possamos construir histogramas para os valores calculados para essas duas quantidades; esses histogramas estimam as distribuições amostrais das estatísticas consideradas.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="estimação-pontual.html#cb80-1"></a>lambda &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">10</span>)  <span class="co"># lambda real (desconhecido)</span></span>
<span id="cb80-2"><a href="estimação-pontual.html#cb80-2"></a>nS     &lt;-<span class="st"> </span><span class="dv">1000</span>     <span class="co"># no. de a.a&#39;s coletadas</span></span>
<span id="cb80-3"><a href="estimação-pontual.html#cb80-3"></a>nobs   &lt;-<span class="st"> </span><span class="dv">30</span>       <span class="co"># no. de obs. em cada amostra </span></span>
<span id="cb80-4"><a href="estimação-pontual.html#cb80-4"></a>                   <span class="co"># (tamanho da a.a.)</span></span>
<span id="cb80-5"><a href="estimação-pontual.html#cb80-5"></a></span>
<span id="cb80-6"><a href="estimação-pontual.html#cb80-6"></a><span class="co"># (1) obteção das amostras aleatórias</span></span>
<span id="cb80-7"><a href="estimação-pontual.html#cb80-7"></a>aa &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rpois</span>(nobs<span class="op">*</span>nS, lambda), <span class="dt">ncol =</span> nobs) </span>
<span id="cb80-8"><a href="estimação-pontual.html#cb80-8"></a><span class="co"># cada uma das &#39;nS&#39; linhas recebe </span></span>
<span id="cb80-9"><a href="estimação-pontual.html#cb80-9"></a><span class="co"># uma amostra de tamanho &#39;nobs&#39;</span></span>
<span id="cb80-10"><a href="estimação-pontual.html#cb80-10"></a></span>
<span id="cb80-11"><a href="estimação-pontual.html#cb80-11"></a><span class="co"># (2) cálculo das estatisticas para as amostras</span></span>
<span id="cb80-12"><a href="estimação-pontual.html#cb80-12"></a>X_bar &lt;-<span class="st"> </span><span class="kw">apply</span>(aa, <span class="dv">1</span>, mean)</span>
<span id="cb80-13"><a href="estimação-pontual.html#cb80-13"></a>Sn2   &lt;-<span class="st"> </span><span class="kw">apply</span>(aa, <span class="dv">1</span>, var) </span>
<span id="cb80-14"><a href="estimação-pontual.html#cb80-14"></a></span>
<span id="cb80-15"><a href="estimação-pontual.html#cb80-15"></a><span class="co"># (3) distribuição amostral das estatisticas</span></span>
<span id="cb80-16"><a href="estimação-pontual.html#cb80-16"></a><span class="co"># a. média amostral:</span></span>
<span id="cb80-17"><a href="estimação-pontual.html#cb80-17"></a><span class="kw">hist</span>(X_bar, <span class="dt">freq =</span> <span class="ot">FALSE</span>,</span>
<span id="cb80-18"><a href="estimação-pontual.html#cb80-18"></a>     <span class="dt">density =</span> <span class="dv">45</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb80-19"><a href="estimação-pontual.html#cb80-19"></a>     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb80-20"><a href="estimação-pontual.html#cb80-20"></a><span class="kw">abline</span>(<span class="dt">v =</span> lambda, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)      <span class="co"># lambda &quot;real&quot; em  vermelho</span></span>
<span id="cb80-21"><a href="estimação-pontual.html#cb80-21"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(X_bar), <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># media dist. amost.em  azul</span></span>
<span id="cb80-22"><a href="estimação-pontual.html#cb80-22"></a></span>
<span id="cb80-23"><a href="estimação-pontual.html#cb80-23"></a><span class="co"># b. variância amostral:</span></span>
<span id="cb80-24"><a href="estimação-pontual.html#cb80-24"></a><span class="kw">hist</span>(Sn2, <span class="dt">freq =</span> <span class="ot">FALSE</span>,</span>
<span id="cb80-25"><a href="estimação-pontual.html#cb80-25"></a>     <span class="dt">density =</span> <span class="dv">45</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb80-26"><a href="estimação-pontual.html#cb80-26"></a>     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb80-27"><a href="estimação-pontual.html#cb80-27"></a><span class="kw">abline</span>(<span class="dt">v =</span> lambda, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)      <span class="co"># lambda &quot;real&quot; em  vermelho</span></span>
<span id="cb80-28"><a href="estimação-pontual.html#cb80-28"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(Sn2), <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)   <span class="co"># media dist. amost. em  azul</span></span></code></pre></div>
<p><img src="10-ch10_files/figure-html/unnamed-chunk-2-1.png" width="100%" /></p>
<p>Os dois gráficos com as distribuições amostrais estimadas para os estimadores média amostral e variância amostral foram obtidos com o código em R fornecido acima. Neles, é possível observar como variam os valores estimados de <span class="math inline">\(\lambda\)</span> para os diferentes estimadores, aplicados a uma grande quantidade de amostras aleatórias distintas, geradas a partir de simulação.</p>
<p>Para algumas amostras é possível perceber que os valores estimados diferiram consideravelmente do valor real de lambda, <span class="math inline">\(\ln10\)</span> que vale aproximadamente 2,3, independentemente do estimador utilizado.</p>
<p>Queremos um estimador que, em média (ou seja, quando obtemos muitas e muitas amostras da população de origem), ofereça resultados satisfatórios, isto é, que não erre de maneira sistemática ao estimar o valor real do parâmetro de interesse.</p>
<p>De certa forma, os dois estimadores considerados (média amostral e variância amostral) são satisfatórios neste sentido. Note que os valores estimados para <span class="math inline">\(\lambda\)</span> parecem flutuar em torno do valor “real” em ambos os casos.</p>
<p>De fato, é possível demonstrar (e isso fica como exercício para casa) que
os valores esperados da média amostral e da variância amostral valem <span class="math inline">\(\lambda\)</span>, ou seja, as distribuições amostrais de ambos os estimadores estão centradas no parâmetro que se quer estimar:</p>
<p><span class="math display">\[E[\bar{X}_n]=\lambda; \qquad E[S^2_n]=\lambda\]</span></p>
<p>Isto significa que os estimadores média amostral e variância amostral não apresentam uma tendência sistemática de produzir estimativas nem maiores (superestimadas) nem menores (subestimadas) do parâmetro de interesse, <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="não-tendeciosidade-exatidão" class="section level3 unnumbered">
<h3>Não-Tendeciosidade (exatidão)</h3>
<p>Esta é uma característica muito desejável em um estimador, chamada não-tendenciosidade ou exatidão.
Um estimador é dito exato, ou não-tendencioso, ou não-viesado, ou não-viciado, quando sua distribuição amostral está centrada no parâmetro que se presta a estimar. Neste caso, o viés, que mede o desvio sistemático deste estimador ao parâmetro populacional, vale zero.</p>
<ul>
<li><p>um estimador não-viesado é aquele que não tende a subestimar ou superestimar o valor real do parâmetro populacional, <span class="math inline">\(\theta\)</span></p></li>
<li><p>a distribuição amostral de <span class="math inline">\(\hat\Theta\)</span> está centrada em <span class="math inline">\(\theta\)</span></p></li>
</ul>
<p><span class="math display">\[E[\hat\Theta] = \theta \quad \Longrightarrow \quad \text{Viés} = b(\hat\Theta) = E[\hat\Theta] - \theta = 0\]</span></p>
<p><img src="img/biased-estimator.png" width="100%" /></p>
<p>Ótimo, quer dizer que encontramos dois estimadores não-tendenciosos para o parâmetro <span class="math inline">\(\lambda\)</span>!</p>
<p>Isso significa que é indiferente escolher um dos dois?</p>
<p>A resposta é NÃO! Existem outras características que também são importantes para um estimador.</p>
<p>Observe novamente os histogramas para os valores obtidos para os dois
estimadores. Qual a diferença entre eles?</p>
<p><img src="10-ch10_files/figure-html/unnamed-chunk-3-1.png" width="100%" /></p>
<p>Ambos apresentam uma certa variabilidade em torno do valor real de <span class="math inline">\(\lambda\)</span>. No entanto, perceba que a variação nos valores calculados para a média amostral é menor que a variação nos valores calculados para a variância amostral. De fato, basta comparar os valores das variâncias amostrais dos dois estimadores, registradas abaixo de cada um dos histogramas. Isto indica que o estimador média amostral produz estimativas, em geral, mais próximas do valor real do parâmetro lambda que a variância amostral. E esta é uma outra característica desejável de um estimador.</p>
</div>
<div id="eficiência-precisão" class="section level3 unnumbered">
<h3>Eficiência (precisão)</h3>
<p>A eficiência é uma medida relativa de precisão de dois estimadores não tendenciosos. É dito mais eficiente o estimador não tendencioso que apresentar menor variância amostral.</p>
<ul>
<li>dados dois estimadores .ired-inline[não-tendenciosos], <span class="math inline">\(\hat\Theta_1\)</span> e <span class="math inline">\(\hat\Theta_2\)</span>, com variâncias amostrais dadas por <span class="math inline">\(S_1^2\)</span> e <span class="math inline">\(S_2^2\)</span>:</li>
</ul>
<p><span class="math inline">\(\qquad \qquad S_1^2 &lt; S_2^2 \quad \Longrightarrow \quad \hat{\Theta}_1\)</span> é mais eficiente que <span class="math inline">\(\hat{\Theta}_2\)</span></p>
<p><img src="img/efficient-estimator.png" width="100%" /></p>
<div id="estimando-p_o-1" class="section level4 unnumbered">
<h4>Estimando <span class="math inline">\(p_o\)</span></h4>
<p>Agora, vamos estimar a quantidade populacional <span class="math inline">\(p_o\)</span>, que corresponde à probabilidade de nenhuma chegada no intervalo de tempo considerado (note que, como esta quantidade populacional é uma probabilidade, é um número entre 0 e 1!).</p>
<p>Consideraremos dois estimadores candidatos:</p>
<p><span class="math display">\[S = \frac{\textsf{no. de observações na a.a. em que }X_i = 0}{n} \qquad \textsf{e} \qquad W = e^{-\bar{X}_n}\]</span></p>
<p>O que podemos esperar desses estimadores?</p>
<p>Novamente, utilizaremos simulação para nos auxiliar a responder a esta pergunta, seguindo o mesmo procedimento realizado anteriormente:</p>
<ul>
<li>supor <span class="math inline">\(\lambda = \ln 10\)</span> (desconhecido) tal que <span class="math inline">\(p_o = 0.1\)</span></li>
<li>amostrar n = 30 observações a partir de Pois ( <span class="math inline">\(\lambda = \ln 10\)</span> )<br />
</li>
<li>calcular valores os que <span class="math inline">\(S\)</span> e <span class="math inline">\(W\)</span> assumem para as observações da a.a.<br />
</li>
<li>repetir esse procedimento um grande número de vezes</li>
<li>construir a distribuição amostral para cada estimador</li>
</ul>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="estimação-pontual.html#cb81-1"></a>lambda &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">10</span>)  <span class="co"># lambda real (desconhecido)</span></span>
<span id="cb81-2"><a href="estimação-pontual.html#cb81-2"></a>po &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>lambda) <span class="co"># po real (desconhecido)</span></span>
<span id="cb81-3"><a href="estimação-pontual.html#cb81-3"></a></span>
<span id="cb81-4"><a href="estimação-pontual.html#cb81-4"></a><span class="co"># Define Estimadores (funcoes de a.a.&#39;s)</span></span>
<span id="cb81-5"><a href="estimação-pontual.html#cb81-5"></a><span class="co"># Estimador freq. relativa de 0&#39;s</span></span>
<span id="cb81-6"><a href="estimação-pontual.html#cb81-6"></a>S_fcn &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">return</span>(<span class="kw">sum</span>(x <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)<span class="op">/</span><span class="kw">length</span>(x))</span>
<span id="cb81-7"><a href="estimação-pontual.html#cb81-7"></a><span class="co"># Estimador e^{-X_bar}</span></span>
<span id="cb81-8"><a href="estimação-pontual.html#cb81-8"></a>W_fcn &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">return</span>(<span class="kw">exp</span>(<span class="op">-</span><span class="kw">mean</span>(x)))</span>
<span id="cb81-9"><a href="estimação-pontual.html#cb81-9"></a></span>
<span id="cb81-10"><a href="estimação-pontual.html#cb81-10"></a><span class="co"># (2) cálculo das estatisticas para as amostras</span></span>
<span id="cb81-11"><a href="estimação-pontual.html#cb81-11"></a>S_hat &lt;-<span class="st"> </span><span class="kw">apply</span>(aa, <span class="dv">1</span>, S_fcn)</span>
<span id="cb81-12"><a href="estimação-pontual.html#cb81-12"></a>W_hat &lt;-<span class="st"> </span><span class="kw">apply</span>(aa, <span class="dv">1</span>, W_fcn) </span>
<span id="cb81-13"><a href="estimação-pontual.html#cb81-13"></a></span>
<span id="cb81-14"><a href="estimação-pontual.html#cb81-14"></a><span class="co"># (3) distribuição amostral das estatisticas</span></span>
<span id="cb81-15"><a href="estimação-pontual.html#cb81-15"></a><span class="co"># a) S</span></span>
<span id="cb81-16"><a href="estimação-pontual.html#cb81-16"></a><span class="kw">hist</span>(S_hat, <span class="dt">freq =</span> <span class="ot">FALSE</span>,</span>
<span id="cb81-17"><a href="estimação-pontual.html#cb81-17"></a>     <span class="dt">density =</span> <span class="dv">45</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb81-18"><a href="estimação-pontual.html#cb81-18"></a>     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb81-19"><a href="estimação-pontual.html#cb81-19"></a><span class="kw">abline</span>(<span class="dt">v =</span> po, <span class="dt">col =</span> <span class="dv">2</span>,  <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)         <span class="co"># po &quot;real&quot; em  vermelho</span></span>
<span id="cb81-20"><a href="estimação-pontual.html#cb81-20"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(S_hat), <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># media dist. amost. em  azul</span></span>
<span id="cb81-21"><a href="estimação-pontual.html#cb81-21"></a><span class="co"># b) W</span></span>
<span id="cb81-22"><a href="estimação-pontual.html#cb81-22"></a><span class="kw">hist</span>(W_hat, <span class="dt">freq =</span> <span class="ot">FALSE</span>,</span>
<span id="cb81-23"><a href="estimação-pontual.html#cb81-23"></a>     <span class="dt">density =</span> <span class="dv">45</span>, <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb81-24"><a href="estimação-pontual.html#cb81-24"></a>     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb81-25"><a href="estimação-pontual.html#cb81-25"></a><span class="kw">abline</span>(<span class="dt">v =</span> po, <span class="dt">col =</span> <span class="dv">2</span>,  <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)         <span class="co"># po &quot;real&quot; em  vermelho</span></span>
<span id="cb81-26"><a href="estimação-pontual.html#cb81-26"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(W_hat), <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># media dist. amost. em  azul</span></span></code></pre></div>
<p><img src="10-ch10_files/figure-html/unnamed-chunk-5-1.png" width="100%" /></p>
<p>Os gráficos com as distribuições amostrais estimadas foram obtidos com o código em R fornecido acima. Os valores estimados para <span class="math inline">\(p_o\)</span> parecem flutuar em torno do valor “real” (0,1) que pretendem estimar. Será verdade? Precisamos calcular o viés destes estimadores.</p>
<p>Definindo:
<span class="math inline">\(Y =\)</span> número de observações em uma amostra aleatória de tamanho <span class="math inline">\(n\)</span> em que <span class="math inline">\(X_i = 0\)</span> (sucesso), então, chegamos à conclusão de que</p>
<p><span class="math display">\[Y \sim Bin(n, p_o)\]</span></p>
<p>É possível demonstrar (e isto fica como exercício para casa) que o estimador S, dado pela proporção de sucessos é não viesado, ou seja tem valor esperado igual a <span class="math inline">\(p_o\)</span>:</p>
<p><span class="math display">\[S = \frac{Y}{n} \qquad \Rightarrow \qquad E[S] = p_o\]</span></p>
<p>Portanto, o estimador <span class="math inline">\(S\)</span> é não-viesado.</p>
<p>Agora, analisemos o estimador <span class="math inline">\(W = e^{-\bar{X}_n}\)</span>. A partir da análise visual da distribuição amostral estimada, Podemos ter a impressão de que <span class="math inline">\(W\)</span> é também um estimador não-viesado. Mas isto não é verdade! É possível mostrar que:
<span class="math display">\[E[W] = E[e^{-\bar{X}_n}] = e^{-n\lambda(1- e^{-1/n})}
  \quad \underset{n \rightarrow \infty}{\longrightarrow} e^{-\lambda} = p_o\]</span></p>
<p>No entanto, embora o estimador <span class="math inline">\(W\)</span> seja tendencioso, ele tem uma característica interessante: é possível mostrar que este estimador produz estimativas que convergem para o valor real de <span class="math inline">\(p_o\)</span>, conforme aumenta o tamanho da amostra <span class="math inline">\(n\)</span>. Sendo assim, embora o estimador seja positivamente viesado (tende a superestimar o parâmetro real), o viés tende a zero à medida em que o tamanho da amostra aumenta.</p>
</div>
</div>
<div id="consistência" class="section level3 unnumbered">
<h3>Consistência</h3>
<p>Diz-se que um estimador é consistente quando as estimativas por ele produzidas convergem em probabilidade para o valor real do parâmetro, conforme aumenta o tamanho da amostra. Isso significa que tanto o viés quanto a variância do estimador tendem a zero, para amostras grandes.</p>
<ul>
<li>à medida que o tamanho da amostra <span class="math inline">\(n\)</span> aumenta, o valor esperado do estimador consistente tende para o parâmetro populacional e a variância do estimador cai a zero:</li>
</ul>
<p><span class="math display">\[\underset{n\rightarrow \infty}{\lim} P[|\hat{\Theta}_n -\theta| &lt; \epsilon] = 1,  \quad \{\hat\Theta_n , n\geq0\}\]</span></p>
<p><img src="img/consistent-estimator.png" width="100%" /></p>
<p>Veja no gráfico abaixo como o valor esperado de <span class="math inline">\(W\)</span> converge para o valor real de <span class="math inline">\(p_o = 0,1\)</span>, conforme tomamos amostras de tamanho crescente.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="estimação-pontual.html#cb82-1"></a>lambda &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">10</span>)  <span class="co"># lambda real</span></span>
<span id="cb82-2"><a href="estimação-pontual.html#cb82-2"></a>po &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>lambda) <span class="co"># po real</span></span>
<span id="cb82-3"><a href="estimação-pontual.html#cb82-3"></a></span>
<span id="cb82-4"><a href="estimação-pontual.html#cb82-4"></a><span class="co"># Valor esperado do estimador e^{-X_bar}</span></span>
<span id="cb82-5"><a href="estimação-pontual.html#cb82-5"></a>W_expectation &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda){ </span>
<span id="cb82-6"><a href="estimação-pontual.html#cb82-6"></a>  <span class="kw">return</span>(<span class="kw">exp</span>(<span class="op">-</span>(x<span class="op">*</span>lambda)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>x))))</span>
<span id="cb82-7"><a href="estimação-pontual.html#cb82-7"></a>  }</span>
<span id="cb82-8"><a href="estimação-pontual.html#cb82-8"></a></span>
<span id="cb82-9"><a href="estimação-pontual.html#cb82-9"></a><span class="co"># Gráfico de E[W] x n</span></span>
<span id="cb82-10"><a href="estimação-pontual.html#cb82-10"></a><span class="kw">plot</span>(<span class="kw">W_expectation</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, lambda), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,</span>
<span id="cb82-11"><a href="estimação-pontual.html#cb82-11"></a>     <span class="dt">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="dt">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb82-12"><a href="estimação-pontual.html#cb82-12"></a>     <span class="dt">ylab =</span> <span class="st">&quot;E [ W(n) ]&quot;</span>,</span>
<span id="cb82-13"><a href="estimação-pontual.html#cb82-13"></a>     <span class="dt">xlab =</span> <span class="st">&quot;tamanho da amostra, n&quot;</span>,</span>
<span id="cb82-14"><a href="estimação-pontual.html#cb82-14"></a>     <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb82-15"><a href="estimação-pontual.html#cb82-15"></a><span class="kw">abline</span>(<span class="dt">h =</span> po, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)   <span class="co"># po &quot;real&quot; em  vermelho</span></span></code></pre></div>
<p><img src="10-ch10_files/figure-html/unnamed-chunk-7-1.png" width="100%" /></p>

<div class="example">
<span id="exm:unnamed-chunk-8" class="example"><strong>Exemplo 10.1  </strong></span>
</div>

<p>Seja <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> uma a.a. de uma distribuição com média <span class="math inline">\(\mu\)</span> e variância <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Responda:</p>
<ol style="list-style-type: decimal">
<li>A estatística <span class="math inline">\(\overline{X}_n\)</span> é um estimador não-tendencioso de <span class="math inline">\(\mu\)</span>?<br />
</li>
<li>A estatística <span class="math inline">\(S_n^2\)</span> é um estimador não-viesado de <span class="math inline">\(\sigma^2\)</span>?<br />
</li>
<li>A estatística <span class="math inline">\(S_n\)</span> é um estimador não-viciado de <span class="math inline">\(\sigma\)</span>?</li>
</ol>
<p><strong>OBS:</strong><br />
Se <span class="math inline">\(\hat{\Theta}\)</span> é um estimador não-viesado de <span class="math inline">\(\theta\)</span>, <span class="math inline">\(g(\hat{\Theta})\)</span> não-necessariamente é um estimador não-viesado de <span class="math inline">\(g(\theta)\)</span>.</p>
<p>Para o caso particular em que <span class="math inline">\(g(\hat{\Theta}) = a\hat{\Theta} + b\)</span>, a não-tendenciosidade é transmitida.</p>

<div class="solution">
 <span class="solution"><em>Solução. </em></span> 
</div>

<p><strong>1)</strong><br />
<span class="math inline">\(X_1, X_2, \ldots, X_n \overset{ind.}{\sim} f_X:\)</span> <span class="math inline">\(E[X_i] = \mu\)</span> e <span class="math inline">\(\textsf{Var}[X_i] = \sigma^2\)</span>, <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<blockquote>
<p>A estatística <span class="math inline">\(\bar{X}_n\)</span> é um estimador não-tendencioso de <span class="math inline">\(\mu\)</span>?</p>
</blockquote>
<p>Temos:
<span class="math display">\[\begin{align*}
  E[\bar{X}_n] = E\left[\frac{1}{n}\sum X_i\right] = \frac{1}{n} \sum E[X_i] = \frac{1}{n} \sum \mu = \frac{1}{n} (n \cdot \mu) = \mu
\end{align*}\]</span></p>
<p>Portanto, <span class="math inline">\(\bar{X}_n\)</span> é um estimador não tendencioso de <span class="math inline">\(\mu\)</span>.</p>
<p><strong>2)</strong><br />
<span class="math inline">\(X_1, X_2, \ldots, X_n \overset{ind.}{\sim} f_X:\)</span> <span class="math inline">\(E[X_i] = \mu\)</span> e <span class="math inline">\(Var[X_i] = \sigma^2\)</span>, <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<blockquote>
<p>A estatística <span class="math inline">\(S_n^2\)</span> é um estimador não-viesado de <span class="math inline">\(\sigma^2\)</span>?</p>
</blockquote>
<p>Temos: <span class="math inline">\(Var[Y] = E[Y^2] - (E[Y])^2 \Rightarrow E[Y^2] = Var[Y] - (E[Y])^2 = \sigma^2 + \mu^2\)</span>
<span class="math display">\[\begin{align*}
  E[S_n^2] 
  &amp;= E\left[\frac{1}{n-1}\sum (X_i - \bar{X})^2\right]\\ 
  &amp;= E\left[\frac{1}{n-1}\left(\sum X_i^2 - \frac{(\sum X_i)^2}{n}\right)\right]\\
  &amp;= \frac{1}{n-1}\left[ \sum E[X_i^2] - \frac{E[(\sum X_i)^2]}{n} \right]
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
  E[S_n^2] 
  &amp;= \frac{1}{n-1}\left[ \sum (\sigma^2 + \mu^2) - \frac{Var[\sum X_i] + (E[\sum X_i])^2}{n}  \right]\\ 
   &amp;= \frac{1}{n-1}\left[n(\sigma^2 + \mu^2) - \frac{1}{n} (n\sigma^2 + (n\mu)^2) \right]\\
  &amp;= \frac{1}{n-1}\left[ n\sigma^2 + n\mu^2 - \sigma^2 - n\mu^2 \right] = \frac{1}{n-1}\left[(n-1)\sigma^2 \right] = \sigma^2
\end{align*}\]</span></p>
<p>Portanto, <span class="math inline">\(S_n^2\)</span> é um estimador não tendencioso de <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Já <span class="math inline">\(S_n\)</span> é um estimador tendencioso de <span class="math inline">\(\sigma\)</span>: <span class="math inline">\(E[S_n] \neq \sigma\)</span> (viés <span class="math inline">\(\downarrow\)</span> quando <span class="math inline">\(n \uparrow\)</span>).</p>
</div>
<div id="erro-médio-quadrático" class="section level3 unnumbered">
<h3>Erro Médio Quadrático</h3>
<p>Nem sempre é possível obter um estimador que seja ao mesmo tempo não-tendencioso e eficiente. Além disso, pode ser difícil comparar um estimador viesado com pequena variância a um estimador não-tendencioso, mas que produz estimativas com grande dispersão.</p>
<p>Portanto, é desejável combinar as propriedades de não tendenciosidade e mínima variância para avaliar a qualidade de um estimador. Esta combinação pode ser obtida através do erro médio quadrático, que corresponde à soma da variância e do viés quadrático. Sendo assim, o erro médio quadrático pode ser utilizado como uma medida relativa de eficiência na comparação de dois ou mais estimadores. Um estimador torna-se mais eficiente a medida que seu erro médio quadrático diminui.</p>
<ul>
<li>O erro médio quadrático (MSE, <em>mean squared error</em>) corresponde ao valor médio dos desvios quadráticos entre <span class="math inline">\(\hat{\Theta}\)</span> e <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p><span class="math display">\[MSE [\hat{\Theta}] = E[(\hat\Theta - \theta)^2] = \mathit{Var}[\hat\Theta] + \left(b[\hat\Theta]\right)^2\]</span></p>
<p><img src="img/MSE.png" width="100%" /></p>

<div class="example">
<span id="exm:unnamed-chunk-10" class="example"><strong>Exemplo 8.1  </strong></span>
</div>

<p>Sejam <span class="math inline">\(\Theta_1\)</span> e <span class="math inline">\(\Theta_2\)</span> estimadores independentes não-tendenciosos de <span class="math inline">\(\theta\)</span>, com variâncias conhecidas dadas por <span class="math inline">\(\sigma_1^2\)</span> e <span class="math inline">\(\sigma_2^2\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Mostre que qualquer estimador na forma
<span class="math display">\[\Theta^\star = \lambda \Theta_1 + (1-\lambda) \Theta_2\]</span>
é também não tendencioso.</p></li>
<li><p>Qual o valor de <span class="math inline">\(\lambda\)</span> que confere a <span class="math inline">\(\Theta^\star\)</span> o menor erro médio quadrático?</p></li>
</ol>
</div>
</div>
<div id="métodos-clássicos-de-estimação-de-parâmetros" class="section level2">
<h2><span class="header-section-number">10.3</span> Métodos Clássicos de Estimação de Parâmetros</h2>
<p>Como vimos anteriormente, em alguns casos é fácil decidir como estimar certos parâmetros populacionais e as escolhas de estimadores podem ser bastante intuitivas, de forma que surgem candidatos naturais a estimadores para uma certa quantidade populacional. No entanto, há situações mais complexas em que apenas a intuição não nos servirá de muito auxílio. É necessário, portanto, estabelecer alguns métodos sistemáticos para construir estimadores para parâmetros populacionais.
Veremos a seguir dois métodos clássicos, o método dos momentos e o método da máxima verossimilhança.</p>
<div id="método-dos-momentos" class="section level3 unnumbered">
<h3>Método dos Momentos</h3>
<p>O método dos momentos é, talvez, o método de construção de estimadores mais antigo a fazer parte da caixa de ferramentas estatísticas, tendo sido desenvolvido por Karl Pearson, no final do século XIX.
Este método de estimação baseia-se na Lei dos Grandes Números que, como vimos anteriormente, afirma que a média amostral converge para o valor esperado populacional conforme o número de observações na amostra aumenta. Sendo assim, na sua forma mais simples, o método dos momentos consiste em uma técnica que nos permite estimar os momentos de uma população com base nos momentos estimados a partir de uma amostra retirada desta população.</p>
<p>Seja <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> i.i.d. <span class="math inline">\(f_X(x| \theta_1, \theta_2, \ldots, \theta_k)\)</span>, com <span class="math inline">\(\theta = (\theta_1, \theta_2, \ldots, \theta_k)\)</span> desconhecido.</p>
<p>O k-ésimo momento populacional é definido como:
<span class="math display">\[\mu_k^\prime(\theta_1, \theta_2, \ldots, \theta_k) = E[X^k]\]</span></p>
<p>O k-ésimo momento amostral (para uma a.a. de tamanho n) é definido como:
<span class="math display">\[m_k^\prime  = \frac{1}{n}\sum_{i=1}^n X_i^k\]</span></p>
<p>O método dos momentos consiste em igualar os <span class="math inline">\(k\)</span> primeiros momentos amostrais aos <span class="math inline">\(k\)</span> primeiros momentos populacionais, ou seja, fazer, <span class="math inline">\(m_k^\prime = \mu_k^\prime(\theta_1, \theta_2, \ldots, \theta_k)\)</span> e resolver para <span class="math inline">\(\theta_1, \theta_2, \ldots, \theta_k\)</span>; temos assim, <span class="math inline">\(k\)</span> equações a <span class="math inline">\(k\)</span> incógnitas (as incógnitas correspondem justamente aos <span class="math inline">\(k\)</span> parâmetros populacionais desconhecidos). Em geral, este procedimento leva a um conjunto de <span class="math inline">\(k\)</span> equações simultâneas não-lineares em <span class="math inline">\(k\)</span> variáveis.</p>
<p>O Método dos Momentos é capaz de gerar estimadores de maneira simples e intuitiva que, na maioria dos casos, são consistentes, ou seja, convergem em probabilidade para o valor real do parâmetro, conforme aumenta o tamanho da amostra. No entanto, em muitos casos, os estimadores obtidos através do método dos momentos não apresentam características desejáveis, podendo ser viesados ou ineficientes. Apesar desses inconvenientes, o método dos momentos é uma ferramenta bastante útil e amplamente utilizada, podendo ser empregado quando outros métodos de estimação de parâmetros mostram-se inviáveis.</p>

<div class="example">
<span id="exm:unnamed-chunk-11" class="example"><strong>Exemplo 5.2  (Distribuição Normal)  </strong></span>
</div>

<p>Vejamos um exemplo de como obter os estimadores para a média e variância populacionais a partir de uma amostra aleatória obtida a partir de uma distribuição Normal, utilizando para isso o Método dos Momentos. Como a distribuição Normal possui dois parâmetros desconhecidos, precisaremos utilizar os dois primeiros momentos.</p>
<p>Seja <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> i.i.d. <span class="math inline">\(N(\mu, \sigma^2)\)</span>, com <span class="math inline">\(\mu,\; \sigma^2\)</span> desconhecidos.</p>
<p>Momentos populacionais: <span class="math inline">\(\mu_1^\prime = \mu \quad \textsf{e} \quad \mu_2^\prime = \sigma^2 + \mu^2\)</span></p>
<p>Portanto, igualando aos momentos amostrais, temos:
<span class="math display">\[\mu = \frac{1}{n} \sum_{i=1}^n X_i \quad \textsf{e} \quad \sigma^2 + \mu^2 = \frac{1}{n} \sum_{i=1}^n X_i^2\]</span></p>
<p>Resolvendo para <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span>, temos:<br />
<span class="math display">\[\begin{align*}
&amp;\hat{\mu} = \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i 
&amp;\hat{\sigma}^2 = S^{\prime 2} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\end{align*}\]</span></p>
<p>Note que o estimador <span class="math inline">\(\hat\sigma^2\)</span> para a variância populacional é viesado, o que significa que o centro de sua distribuição amostral não coincide com o parâmetro <span class="math inline">\(\sigma^2\)</span> que ele se presta a estimar. Você consegue determinar o viés deste estimador?</p>
</div>
<div id="método-da-máxima-verossimilhança" class="section level3 unnumbered">
<h3>Método da Máxima Verossimilhança</h3>
<p>O Método da Máxima Verossimilhança é, sem dúvida, o mais utilizado para construir estimadores. O princípio em que se baseia e a derivação das propriedades dos estimadores obtidos por este método são resultado do trabalho de sir Ronald Fisher, na primeira metade do século XX. A lógica de Fisher pode ser entendida considerando o seguinte raciocínio:</p>
<p>Um conjunto de <span class="math inline">\(n\)</span> observações <span class="math inline">\(X_1, X_2,\ldots X_n\)</span>, obtidas de maneira independente de uma população comum <span class="math inline">\(f_X(x| \theta_1, \theta_2, \ldots, \theta_k)\)</span>, com <span class="math inline">\(\theta = (\theta_1, \theta_2, \ldots, \theta_k)\)</span> desconhecido, constitui uma a.a. desta população.</p>
<p>Então a probabilidade de observar estes conjunto de valores independentes de <span class="math inline">\(X\)</span> é dada pela distribuição conjunta <span class="math inline">\(f(X_1, X_2, \ldots, X_n|\theta)\)</span>, que é igual ao produto das distribuições marginais, já que as variáveis aleatórias <span class="math inline">\(X_1, X_2, \ldots X_n\)</span> são independentes.</p>
<p>Para uma determinada amostra, esta distribuição conjunta é uma função apenas dos parâmetros populacionais desconhecidos dado pelo vetor <span class="math inline">\(\theta\)</span>, a que Fisher chamou de <strong>Função de Verossimilhança</strong>.</p>
<p>A função de verossimilhança para uma a.a. é dada por:</p>
<p><span class="math display">\[f(X_1, X_2, \ldots, X_n| \theta) = \prod_{i = 1}^{n} f(X_i| \theta) = L(\theta)\]</span></p>
<p>Ele sugeriu então, que o valor de <span class="math inline">\(\theta\)</span> deve ser tal que maximiza a função de verossimilhança, de forma que a amostra obtida seja uma amostra bastante provável, dado o parâmetro estimado para esta população, não uma amostra rara.</p>
<p>O objetivo, então, é maximizar a função <span class="math inline">\(L(\theta)\)</span> para um certo conjunto de dados (uma amostra). Em muitos casos, este problema se reduz a um exercício simples de cálculo diferencial. Note que a primeira derivada nula é uma condição necessária, mas não suficiente, para garantir que um máximo foi encontrado.</p>
<p>Portanto, o estimador de máxima verossimilhança é aquele que maximiza a probabilidade de obter os dados observados:
<span class="math display">\[\frac{\partial L(\theta)}{\partial \theta_i}=0, i = 1, \ldots, k \quad \Rightarrow \quad  \hat\theta(\mathbf{x}) = \arg \underset{\theta}{\max} L(\theta|\mathbf{x})\]</span></p>
<p>Em geral, os estimadores de máxima verossimilhança coincidem com os estimadores obtidos pelo Método dos Momentos, embora o conjunto de equações simultâneas obtidas pelo Método da Máxima Verossimilhança seja mais difícil de resolver que aquele obtido para o Método dos Momentos.</p>
<p>Fisher mostrou, ainda, que para amostras grandes os estimadores de máxima verossimilhança costumam apresentar características muito desejáveis:</p>
<ul>
<li>são assimptoticamente não tendenciosos (consistentes);<br />
</li>
<li>são assimptoticamente eficientes, de forma que apresentam a menor variância dentre todos os estimadores não viesados (BUE, “best unbiased estimators”); e, além disso,</li>
<li>respeitam o <strong>Princípio da Invariância</strong>:<br />
<span class="math inline">\(\hat\theta(\mathbf{x})\)</span> é estimador de MV de <span class="math inline">\(\theta \Rightarrow g(\hat\theta(\mathbf{x}))\)</span> é estimador de MV de <span class="math inline">\(g(\theta)\)</span></li>
</ul>
<p>Uma observação final, digna de nota, é a seguinte: ao utilizar o princípio da máxima verossimilhança, estamos assumindo que existe um máximo global para esta função. Especialmente para dimensões elevadas (isto é, quando há diversos parâmetros a serem estimados), a função de verossimilhança pode possuir diversos ótimos locais, de forma que encontrar o máximo global pode se converter em um problema computacional complexo.</p>
<p>Para observações independentes, como vimos, a função de verossimilhança consiste no produto das distribuições marginais. O máximo desta função pode ser obtido igualando as diferenciais parciais de <span class="math inline">\(L\)</span> com relação a cada parâmetro desconhecido a zero. Obter esta solução pode ser difícil, mesmo quando temos apenas um parâmetro a ser estimado. Como o <span class="math inline">\(\log L\)</span> é uma função monotônica crescente de <span class="math inline">\(L\)</span>, resolver a equação obtida igualando a derivada do logaritmo da função de verossimilhança a zero é uma tarefa mais simples, que resulta na mesma solução. Assim, para maximizar a função de verossimilhança, é comum fazer <span class="math inline">\(\partial\log L(\theta)/\partial\theta_i = 0\)</span>, em vez de <span class="math inline">\(\partial L(\theta)/\partial \theta_i = 0, \; i = 1, \ldots, k\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-12" class="example"><strong>Exemplo 7.2  (Distribuição Normal)  </strong></span>
</div>

<p>Assim como fizemos para o método dos momentos, vejamos um exemplo de como obter os estimadores de máxima verossimilhança para a média e variância populacionais a partir de uma amostra aleatória obtida a partir de uma distribuição Normal.</p>
<p>Sejam <span class="math inline">\(X_1, X_2, \ldots, X_n \overset{i.i.d.}{\sim} N(\mu, \sigma^2)\)</span>.</p>
<p>A função de verossimilhança para esta a.a. é:</p>
<p><span class="math display">\[L(\mu, \sigma^2) = \prod_{i = 1}^{n} N(X_i; \mu, \sigma^2) 
                   = \left( \frac{1}{\sigma\sqrt{2\pi}} \right)^n
                     \exp\left[ -\frac{1}{2\sigma^2} \sum_{i =1}^{n} (X_i - \mu)^2\right]\]</span></p>
<p>Tomando as derivadas parciais do logaritmo de <span class="math inline">\(L\)</span> ( <span class="math inline">\(l = \log L\)</span> ) com respeito a <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span>, chegamos ao seguinte sistema de equações a duas incógnitas:</p>
<p><span class="math display">\[\begin{align*}
  \frac{\partial l}{\partial \mu} = \frac{1}{\sigma^2} \sum(X_i - \mu) =0\\
  \frac{\partial l}{\partial \sigma^2} = \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum(X_i - \mu)^2 =0
\end{align*}\]</span></p>
<p>Resolvendo para <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span>, obtemos:</p>
<p><span class="math display">\[\begin{align*}
&amp;\hat{\mu} = \bar{X} = \frac{1}{n} \sum X_i \\
&amp;\hat{\sigma}^2 = S^{\prime 2} = \frac{1}{n} \sum (X_i - \bar{X})^2
\end{align*}\]</span></p>
<p>Lembre-se que é necessário verificar se estes correspondem a pontos de máximo, através da avaliação das segundas derivadas.</p>
<p>Note que, neste caso, os estimadores de máxima verossimilhança para média e variância populacionais de uma distribuição Normal são os mesmos daqueles obtidos pelo Método dos Momentos, embora isso nem sempre ocorra.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distribuições-amostrais.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervalos-de-confiança.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["apostila-GED13.pdf", "apostila-GED13.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
